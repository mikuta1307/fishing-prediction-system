#!/usr/bin/env python3
# -*- coding: utf-8 -*-
"""
æœ¬ç‰§æµ·é‡£ã‚Šæ–½è¨­ ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å…±é€šå‡¦ç†ï¼ˆGoogle Sheetsç‰ˆ - è¿½è¨˜æ©Ÿèƒ½å¯¾å¿œï¼‰
ãƒ­ãƒ¼ã‚«ãƒ«ã¨Vercelä¸¡æ–¹ã§ä½¿ç”¨ã•ã‚Œã‚‹å…±é€šãƒ­ã‚¸ãƒƒã‚¯ + Google SheetsæŠ•å…¥æ©Ÿèƒ½ï¼ˆé‡è¤‡é˜²æ­¢å¯¾å¿œï¼‰+ è©³ç´°ãƒ­ã‚°
"""

import time
import os
import re
import csv
import pandas as pd
from datetime import datetime
from selenium import webdriver
from selenium.webdriver.common.by import By
from selenium.webdriver.support.ui import WebDriverWait
from selenium.webdriver.support import expected_conditions as EC
from selenium.webdriver.chrome.options import Options
from selenium.webdriver.chrome.service import Service
from selenium.common.exceptions import TimeoutException, NoSuchElementException

# Google Sheetsé–¢é€£ã®ã‚¤ãƒ³ãƒãƒ¼ãƒˆ
try:
    import gspread
    from google.oauth2.service_account import Credentials
    GOOGLE_SHEETS_AVAILABLE = True
except ImportError:
    GOOGLE_SHEETS_AVAILABLE = False
    print("âš ï¸ Google Sheetsé–¢é€£ãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“ã€‚ä»¥ä¸‹ã‚’å®Ÿè¡Œã—ã¦ãã ã•ã„ï¼š")
    print("pip install gspread google-auth google-auth-oauthlib google-auth-httplib2")

class ScrapingCore:
    """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å…±é€šå‡¦ç†ã‚¯ãƒ©ã‚¹"""
    
    def __init__(self, headless=True):
        """åˆæœŸåŒ–"""
        self.driver = None
        self.target_url = "https://yokohama-fishingpiers.jp/honmoku/fishing-history"
        self.headless = headless
        
        # ç’°å¢ƒåˆ¤å®šï¼ˆãƒ­ãƒ¼ã‚«ãƒ« or Vercelï¼‰
        self.is_vercel = os.environ.get('VERCEL') == '1'
        
        if self.is_vercel:
            # Vercelç’°å¢ƒï¼ˆè‡ªå‹•WebDriverï¼‰
            print("ğŸŒ Vercelç’°å¢ƒã§å®Ÿè¡Œä¸­")
        else:
            # ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒï¼ˆChrome for Testingï¼‰
            print("ğŸ  ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒã§å®Ÿè¡Œä¸­")
            self.chrome_path = r"C:\Users\kataoka.akihito\Downloads\tools\chrome\chrome-win64_Stable_133.0.6943.98\chrome-win64\chrome.exe"
            self.chromedriver_path = r"C:\Users\kataoka.akihito\Downloads\tools\chrome\chromedriver-win64_Stable_133.0.6943.98\chromedriver-win64\chromedriver.exe"
    
    def setup_driver(self):
        """WebDriveråˆæœŸåŒ–ï¼ˆç’°å¢ƒè‡ªå‹•åˆ¤å®šï¼‰"""
        try:
            print("ğŸ”§ Chrome WebDriverã‚’æº–å‚™ä¸­...")
            
            options = Options()
            if self.headless:
                options.add_argument('--headless')
            
            options.add_argument('--no-sandbox')
            options.add_argument('--disable-dev-shm-usage')
            options.add_argument('--disable-gpu')
            options.add_argument('--window-size=1920,1080')
            options.add_argument('--user-agent=Mozilla/5.0 (Windows NT 10.0; Win64; x64) AppleWebKit/537.36 (KHTML, like Gecko) Chrome/120.0.0.0 Safari/537.36')
            
            if self.is_vercel:
                # Vercelç’°å¢ƒ: è‡ªå‹•WebDriver
                self.driver = webdriver.Chrome(options=options)
            else:
                # ãƒ­ãƒ¼ã‚«ãƒ«ç’°å¢ƒ: æ‰‹å‹•ãƒ‘ã‚¹æŒ‡å®š
                options.binary_location = self.chrome_path
                service = Service(executable_path=self.chromedriver_path)
                self.driver = webdriver.Chrome(service=service, options=options)
            
            self.driver.implicitly_wait(10)
            print("âœ… WebDriveråˆæœŸåŒ–å®Œäº†")
            return True
            
        except Exception as e:
            print(f"âŒ WebDriveråˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}")
            return False
    
    def scrape_period(self, target_year_month):
        """æŒ‡å®šå¹´æœˆã®é‡£æœãƒ‡ãƒ¼ã‚¿ã‚’ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ"""
        try:
            # å¹´æœˆè§£æ
            year = int(target_year_month[:4])
            month = int(target_year_month[4:])
            
            print(f"ğŸ£ é‡£æœãƒ‡ãƒ¼ã‚¿æŠ½å‡ºé–‹å§‹: {year}å¹´{month:02d}æœˆ")
            
            print(f"ğŸ—“ï¸ {year}å¹´{month:02d}æœˆã®ãƒ‡ãƒ¼ã‚¿ã‚’å‡¦ç†ä¸­...")
            
            # æ¤œç´¢ãƒ•ã‚©ãƒ¼ãƒ ã§å¹´æœˆã‚’è¨­å®š
            if not self._set_search_period(year, month):
                print(f"âŒ {year}å¹´{month:02d}æœˆã®æ¤œç´¢è¨­å®šã«å¤±æ•—")
                return {
                    'success': False,
                    'error': f'{year}å¹´{month:02d}æœˆã®æ¤œç´¢è¨­å®šã«å¤±æ•—'
                }
            
            # æ¤œç´¢å®Ÿè¡Œ
            if not self._execute_search():
                print(f"âŒ {year}å¹´{month:02d}æœˆã®æ¤œç´¢å®Ÿè¡Œã«å¤±æ•—")
                return {
                    'success': False,
                    'error': f'{year}å¹´{month:02d}æœˆã®æ¤œç´¢å®Ÿè¡Œã«å¤±æ•—'
                }
            
            # æ¤œç´¢çµæœã‹ã‚‰ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºï¼ˆé‡£æœã¨ã‚³ãƒ¡ãƒ³ãƒˆã‚’åˆ†é›¢ï¼‰
            fishing_data, comment_data = self._extract_monthly_data(year, month)
            if fishing_data:
                print(f"âœ… {year}å¹´{month:02d}æœˆå®Œäº† - é‡£æœ: {len(fishing_data)}ä»¶, ã‚³ãƒ¡ãƒ³ãƒˆ: {len(comment_data)}ä»¶")
                
                # CSVãƒ•ã‚¡ã‚¤ãƒ«å‡ºåŠ›
                fishing_csv = f"fishing_results_{target_year_month}.csv"
                comment_csv = f"daily_comments_{target_year_month}.csv"
                
                # é‡£æœãƒ‡ãƒ¼ã‚¿CSV
                fishing_headers = ['æ—¥ä»˜', 'å¤©æ°—', 'æ°´æ¸©', 'æ½®', 'æ¥å ´è€…æ•°', 'é­šç¨®', 'é‡£æœæ•°', 'ã‚µã‚¤ã‚º', 'é‡£ã‚Šå ´']
                self._save_to_csv(fishing_data, fishing_headers, fishing_csv)
                
                # ã‚³ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿CSV
                comment_headers = ['æ—¥ä»˜', 'å¤©æ°—', 'æ°´æ¸©', 'æ½®', 'æ¥å ´è€…æ•°', 'ã‚³ãƒ¡ãƒ³ãƒˆ']
                self._save_to_csv(comment_data, comment_headers, comment_csv)
                
                print(f"ğŸ’¾ é‡£æœãƒ‡ãƒ¼ã‚¿CSVå‡ºåŠ›å®Œäº†: {fishing_csv}")
                print(f"ğŸ’¾ ã‚³ãƒ¡ãƒ³ãƒˆCSVå‡ºåŠ›å®Œäº†: {comment_csv}")
                
                return {
                    'success': True,
                    'total_records': len(fishing_data),
                    'fishing_csv': fishing_csv,
                    'comment_csv': comment_csv,
                    'period': f"{year}å¹´{month:02d}æœˆ"
                }
            else:
                print(f"âš ï¸ {year}å¹´{month:02d}æœˆ - ãƒ‡ãƒ¼ã‚¿ãªã—")
                return {
                    'success': True,
                    'total_records': 0,
                    'fishing_csv': None,
                    'comment_csv': None,
                    'period': f"{year}å¹´{month:02d}æœˆ"
                }
            
        except Exception as e:
            print(f"âŒ é‡£æœãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return {
                'success': False,
                'error': str(e)
            }
    
    def access_site(self):
        """ã‚µã‚¤ãƒˆã‚¢ã‚¯ã‚»ã‚¹"""
        try:
            print(f"ğŸŒ ã‚µã‚¤ãƒˆã‚¢ã‚¯ã‚»ã‚¹ä¸­: {self.target_url}")
            self.driver.get(self.target_url)
            
            # ãƒšãƒ¼ã‚¸ãƒ­ãƒ¼ãƒ‰å®Œäº†ã‚’å¾…æ©Ÿ
            WebDriverWait(self.driver, 15).until(
                EC.presence_of_element_located((By.TAG_NAME, "body"))
            )
            
            title = self.driver.title
            current_url = self.driver.current_url
            
            print(f"âœ… ã‚µã‚¤ãƒˆã‚¢ã‚¯ã‚»ã‚¹æˆåŠŸ")
            print(f"ğŸ“„ ãƒšãƒ¼ã‚¸ã‚¿ã‚¤ãƒˆãƒ«: {title}")
            print(f"ğŸ”— ç¾åœ¨ã®URL: {current_url}")
            
            return True
            
        except Exception as e:
            print(f"âŒ ã‚µã‚¤ãƒˆã‚¢ã‚¯ã‚»ã‚¹ã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def _set_search_period(self, year, month):
        """æ¤œç´¢ãƒ•ã‚©ãƒ¼ãƒ ã§å¹´æœˆã‚’è¨­å®š"""
        try:
            print(f"ğŸ“… æ¤œç´¢æœŸé–“è¨­å®š: {year}å¹´{month:02d}æœˆ")
            
            # å¹´é¸æŠ
            year_dropdown = self.driver.find_element(By.CSS_SELECTOR, ".v-select__menu-icon")
            year_dropdown.click()
            time.sleep(1)
            
            year_option = self.driver.find_element(By.XPATH, f"//div[contains(@class, 'v-list-item') and text()='{year}']")
            year_option.click()
            time.sleep(1)
            
            # æœˆé¸æŠ
            month_dropdowns = self.driver.find_elements(By.CSS_SELECTOR, ".v-select__menu-icon")
            month_dropdowns[1].click()
            time.sleep(1)
            
            month_str = f"{month:02d}"
            month_option = self.driver.find_element(By.XPATH, f"//div[contains(@class, 'v-list-item') and text()='{month_str}']")
            month_option.click()
            time.sleep(1)
            
            return True
            
        except Exception as e:
            print(f"âŒ æ¤œç´¢æœŸé–“è¨­å®šã‚¨ãƒ©ãƒ¼: {e}")
            return False
            
    def _execute_search(self):
        """æ¤œç´¢ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯ã—ã¦çµæœã‚’å¾…æ©Ÿ"""
        try:
            # æ¤œç´¢ãƒœã‚¿ãƒ³ã‚’ã‚¯ãƒªãƒƒã‚¯
            search_button = WebDriverWait(self.driver, 10).until(
                EC.element_to_be_clickable((By.CSS_SELECTOR, "button.fish-search-btn"))
            )
            search_button.click()
            print("ğŸ” æ¤œç´¢å®Ÿè¡Œä¸­...")
            
            # æ¤œç´¢çµæœã®å‡ºç¾ã‚’å¾…æ©Ÿ
            WebDriverWait(self.driver, 15).until(
                EC.presence_of_element_located((By.CSS_SELECTOR, "div.searched-item"))
            )
            time.sleep(2)  # è¿½åŠ å¾…æ©Ÿ
            
            print("âœ… æ¤œç´¢çµæœãƒ­ãƒ¼ãƒ‰å®Œäº†")
            return True
            
        except TimeoutException:
            print("âš ï¸ æ¤œç´¢çµæœã®èª­ã¿è¾¼ã¿ãŒã‚¿ã‚¤ãƒ ã‚¢ã‚¦ãƒˆ")
            return False
        except Exception as e:
            print(f"âŒ æ¤œç´¢å®Ÿè¡Œã‚¨ãƒ©ãƒ¼: {e}")
            return False

    def _extract_monthly_data(self, year, month):
        """æ¤œç´¢çµæœã‹ã‚‰æœˆæ¬¡ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡ºï¼ˆé‡£æœã¨ã‚³ãƒ¡ãƒ³ãƒˆã‚’åˆ†é›¢ï¼‰"""
        try:
            fishing_data_list = []  # é‡£æœãƒ‡ãƒ¼ã‚¿ç”¨
            comment_data_list = []  # ã‚³ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ç”¨
            
            # searched-itemã‚’å–å¾—
            searched_items = self.driver.find_elements(By.CSS_SELECTOR, "div.searched-item")
            
            for item in searched_items:
                # æ—¥ä»˜ã‚’æŠ½å‡º
                date_text = item.text.split('\n')[0].strip()
                
                # ç’°å¢ƒæƒ…å ±ã‚’æŠ½å‡ºï¼ˆå…±é€šï¼‰
                env_data = self._extract_environment_data(item)
                base_data = {
                    'æ—¥ä»˜': date_text,
                    **env_data
                }
                
                # é‡£æœãƒ†ãƒ¼ãƒ–ãƒ«ã‚’æŠ½å‡º
                fishing_data = self._extract_fishing_table(item)
                
                # ã‚³ãƒ¡ãƒ³ãƒˆã‚’æŠ½å‡º
                comment = self._extract_comment(item)
                
                # é‡£æœãƒ‡ãƒ¼ã‚¿ä½œæˆï¼ˆå„é­šç¨®ã”ã¨ã«è¡Œã‚’ä½œæˆã€ã‚³ãƒ¡ãƒ³ãƒˆã¯é™¤å¤–ï¼‰
                for fish in fishing_data:
                    fishing_row = {**base_data, **fish}
                    # ã‚³ãƒ¡ãƒ³ãƒˆã‚­ãƒ¼ãŒå«ã¾ã‚Œã¦ã„ã‚‹å ´åˆã¯é™¤å¤–
                    fishing_row.pop('ã‚³ãƒ¡ãƒ³ãƒˆ', None)
                    fishing_data_list.append(fishing_row)
                
                # ã‚³ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ä½œæˆï¼ˆ1æ—¥1è¡Œã€ã‚³ãƒ¡ãƒ³ãƒˆãŒã‚ã‚‹å ´åˆã®ã¿ï¼‰
                if comment.strip():
                    comment_row = {**base_data, 'ã‚³ãƒ¡ãƒ³ãƒˆ': comment}
                    comment_data_list.append(comment_row)
            
            return fishing_data_list, comment_data_list
            
        except Exception as e:
            print(f"âŒ æœˆæ¬¡ãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return [], []

    def _extract_environment_data(self, item):
        """ç’°å¢ƒæƒ…å ±ï¼ˆå¤©æ°—ã€æ°´æ¸©ãªã©ï¼‰ã‚’æŠ½å‡º"""
        try:
            env_data = {'å¤©æ°—': '', 'æ°´æ¸©': '', 'æ½®': '', 'æ¥å ´è€…æ•°': ''}
            
            status_chips = item.find_elements(By.CSS_SELECTOR, "span.status-chip")
            
            for chip in status_chips:
                text = chip.text.strip()
                if 'å¤©æ°— :' in text:
                    env_data['å¤©æ°—'] = text.replace('å¤©æ°— :', '').split('/')[0].strip()
                elif 'æ°´æ¸© :' in text:
                    env_data['æ°´æ¸©'] = text.replace('æ°´æ¸© :', '').split('/')[0].strip()
                elif 'æ½® :' in text:
                    env_data['æ½®'] = text.replace('æ½® :', '').split('/')[0].strip()
                elif 'æ¥å ´è€…æ•° :' in text:
                    env_data['æ¥å ´è€…æ•°'] = text.replace('æ¥å ´è€…æ•° :', '').split('/')[0].strip()
            
            return env_data
            
        except Exception as e:
            print(f"âŒ ç’°å¢ƒãƒ‡ãƒ¼ã‚¿æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return {'å¤©æ°—': '', 'æ°´æ¸©': '', 'æ½®': '', 'æ¥å ´è€…æ•°': ''}

    def _extract_fishing_table(self, item):
        """é‡£æœãƒ†ãƒ¼ãƒ–ãƒ«ã‹ã‚‰ãƒ‡ãƒ¼ã‚¿ã‚’æŠ½å‡º"""
        try:
            fishing_data = []
            
            # ãƒ†ãƒ¼ãƒ–ãƒ«ã®è¡Œã‚’å–å¾—
            rows = item.find_elements(By.CSS_SELECTOR, "table.fish-list-tabel tbody tr")
            
            i = 0
            while i < len(rows):
                row = rows[i]
                
                # ãƒ¡ã‚¤ãƒ³ã®é‡£æœãƒ‡ãƒ¼ã‚¿è¡Œ
                if not 'sp-place' in row.get_attribute('class'):
                    cells = row.find_elements(By.TAG_NAME, "td")
                    if len(cells) >= 4:
                        # é­šç¨®å
                        fish_name = cells[0].text.strip()
                        
                        # é‡£æœæ•°ï¼ˆæ­£è¦è¡¨ç¾ã§æ•°å€¤æŠ½å‡ºï¼‰
                        count_text = cells[1].text.strip()
                        count_match = re.search(r'(\d+)', count_text)
                        fish_count = count_match.group(1) if count_match else '0'
                        
                        # ã‚µã‚¤ã‚º
                        fish_size = cells[2].text.strip()
                        
                        # é‡£ã‚Šå ´ï¼ˆãƒ†ã‚­ã‚¹ãƒˆã‚’ãã®ã¾ã¾å–å¾—ï¼‰
                        fishing_area = cells[3].text.strip()
                        
                        fishing_data.append({
                            'é­šç¨®': fish_name,
                            'é‡£æœæ•°': fish_count,
                            'ã‚µã‚¤ã‚º': fish_size,
                            'é‡£ã‚Šå ´': fishing_area
                        })
                
                i += 1
            
            return fishing_data
            
        except Exception as e:
            print(f"âŒ é‡£æœãƒ†ãƒ¼ãƒ–ãƒ«æŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return []

    def _extract_comment(self, item):
        """ã‚³ãƒ¡ãƒ³ãƒˆãƒ†ã‚­ã‚¹ãƒˆã‚’æŠ½å‡º"""
        try:
            comment_elements = item.find_elements(By.CSS_SELECTOR, "div.sentence")
            if comment_elements:
                return comment_elements[0].text.strip()
            return ""
        except Exception as e:
            print(f"âŒ ã‚³ãƒ¡ãƒ³ãƒˆæŠ½å‡ºã‚¨ãƒ©ãƒ¼: {e}")
            return ""

    def _save_to_csv(self, data, headers, filename):
        """ãƒ‡ãƒ¼ã‚¿ã‚’CSVãƒ•ã‚¡ã‚¤ãƒ«ã«ä¿å­˜"""
        try:
            with open(filename, 'w', newline='', encoding='utf-8') as csvfile:
                writer = csv.DictWriter(csvfile, fieldnames=headers)
                writer.writeheader()
                writer.writerows(data)
            print(f"ğŸ’¾ {len(data)}ä»¶ã®ãƒ‡ãƒ¼ã‚¿ã‚’{filename}ã«ä¿å­˜ã—ã¾ã—ãŸ")
        except Exception as e:
            print(f"âŒ CSVä¿å­˜ã‚¨ãƒ©ãƒ¼: {e}")
    
    def cleanup(self):
        """ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—"""
        try:
            if self.driver:
                self.driver.quit()
                print("âœ… WebDriverã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—å®Œäº†")
        except Exception as e:
            print(f"âŒ ã‚¯ãƒªãƒ¼ãƒ³ã‚¢ãƒƒãƒ—ã‚¨ãƒ©ãƒ¼: {e}")

# =============================================================================
# Google SheetsæŠ•å…¥æ©Ÿèƒ½ï¼ˆè¿½è¨˜æ©Ÿèƒ½å¯¾å¿œç‰ˆï¼‰
# =============================================================================

def setup_google_sheets_client():
    """Google Sheets ã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–"""
    try:
        if not GOOGLE_SHEETS_AVAILABLE:
            return None, "Google Sheetsãƒ©ã‚¤ãƒ–ãƒ©ãƒªãŒåˆ©ç”¨ã§ãã¾ã›ã‚“"
        
        # èªè¨¼ãƒ•ã‚¡ã‚¤ãƒ«ãƒ‘ã‚¹
        credentials_path = r"C:\Users\kataoka.akihito\Documents\MyPython\env313\choka-389510-1103575d64ab.json"
        
        if not os.path.exists(credentials_path):
            return None, f"èªè¨¼ãƒ•ã‚¡ã‚¤ãƒ«ãŒè¦‹ã¤ã‹ã‚Šã¾ã›ã‚“: {credentials_path}"
        
        # Google Sheets API ã®ã‚¹ã‚³ãƒ¼ãƒ—
        scope = [
            'https://spreadsheets.google.com/feeds',
            'https://www.googleapis.com/auth/drive'
        ]
        
        # ã‚µãƒ¼ãƒ“ã‚¹ã‚¢ã‚«ã‚¦ãƒ³ãƒˆèªè¨¼
        credentials = Credentials.from_service_account_file(credentials_path, scopes=scope)
        client = gspread.authorize(credentials)
        
        return client, None
        
    except Exception as e:
        return None, f"Google Sheetsã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {e}"

def parse_date_for_sort(date_str):
    """æ—¥ä»˜æ–‡å­—åˆ—ã‚’ã‚½ãƒ¼ãƒˆç”¨ã®æ—¥ä»˜ã‚ªãƒ–ã‚¸ã‚§ã‚¯ãƒˆã«å¤‰æ›"""
    try:
        # "2025/07/28(æœˆ)" -> "2025/07/28"
        date_part = date_str.split('(')[0]
        return datetime.strptime(date_part, '%Y/%m/%d')
    except:
        # ãƒ‘ãƒ¼ã‚¹ã§ããªã„å ´åˆã¯æœ€å°æ—¥ä»˜ã‚’è¿”ã™
        return datetime(1900, 1, 1)

def append_data_to_worksheet(worksheet, new_df, sheet_type="ãƒ‡ãƒ¼ã‚¿"):
    """ãƒ¯ãƒ¼ã‚¯ã‚·ãƒ¼ãƒˆã«ãƒ‡ãƒ¼ã‚¿ã‚’è¿½è¨˜ï¼ˆé‡è¤‡ãƒã‚§ãƒƒã‚¯ä»˜ãï¼‰"""
    try:
        print(f"ğŸ“‹ {sheet_type}ã®æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’ãƒã‚§ãƒƒã‚¯ä¸­...")
        
        # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã‚’å–å¾—
        existing_data = worksheet.get_all_records()
        
        if existing_data:
            existing_df = pd.DataFrame(existing_data)
            print(f"ğŸ“Š æ—¢å­˜{sheet_type}: {len(existing_df)}è¡Œ")
            
            # é‡è¤‡ãƒã‚§ãƒƒã‚¯ï¼ˆæ—¥ä»˜ãƒ™ãƒ¼ã‚¹ï¼‰
            if sheet_type == "é‡£æœãƒ‡ãƒ¼ã‚¿":
                # é‡£æœãƒ‡ãƒ¼ã‚¿ã®å ´åˆ: æ—¥ä»˜+é­šç¨®ã§é‡è¤‡ãƒã‚§ãƒƒã‚¯
                existing_keys = set(
                    existing_df['æ—¥ä»˜'].astype(str) + '_' + existing_df['é­šç¨®'].astype(str)
                )
                new_keys = new_df['æ—¥ä»˜'].astype(str) + '_' + new_df['é­šç¨®'].astype(str)
                mask = ~new_keys.isin(existing_keys)
            else:
                # ã‚³ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿ã®å ´åˆ: æ—¥ä»˜ã®ã¿ã§é‡è¤‡ãƒã‚§ãƒƒã‚¯
                existing_keys = set(existing_df['æ—¥ä»˜'].astype(str))
                new_keys = new_df['æ—¥ä»˜'].astype(str)
                mask = ~new_keys.isin(existing_keys)
            
            # æ–°è¦ãƒ‡ãƒ¼ã‚¿ã®ã¿ã‚’æŠ½å‡º
            unique_new_df = new_df[mask].copy()
            
            if len(unique_new_df) == 0:
                print(f"âš ï¸ æ–°è¦{sheet_type}ãªã—ï¼ˆã™ã¹ã¦æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¨é‡è¤‡ï¼‰")
                return 0
            
            print(f"âœ… æ–°è¦{sheet_type}: {len(unique_new_df)}è¡Œï¼ˆé‡è¤‡é™¤å¤–: {len(new_df) - len(unique_new_df)}è¡Œï¼‰")
            
            # æ—¢å­˜ãƒ‡ãƒ¼ã‚¿ã¨æ–°è¦ãƒ‡ãƒ¼ã‚¿ã‚’çµåˆ
            combined_df = pd.concat([existing_df, unique_new_df], ignore_index=True)
        else:
            print(f"ğŸ“ {sheet_type}ã‚·ãƒ¼ãƒˆãŒç©º - å…¨ãƒ‡ãƒ¼ã‚¿ã‚’è¿½åŠ ")
            combined_df = new_df.copy()
            unique_new_df = new_df.copy()
        
        # æ—¥ä»˜é †ã§ã‚½ãƒ¼ãƒˆ
        print(f"ğŸ”„ {sheet_type}ã‚’æ—¥ä»˜é †ã§ã‚½ãƒ¼ãƒˆä¸­...")
        combined_df['_sort_date'] = combined_df['æ—¥ä»˜'].apply(parse_date_for_sort)
        combined_df = combined_df.sort_values('_sort_date').drop('_sort_date', axis=1)
        
        # ã‚·ãƒ¼ãƒˆå…¨ä½“ã‚’æ›´æ–°ï¼ˆãƒ˜ãƒƒãƒ€ãƒ¼å«ã‚€ï¼‰
        print(f"ğŸ“¤ {sheet_type}ã‚·ãƒ¼ãƒˆå…¨ä½“ã‚’æ›´æ–°ä¸­...")
        worksheet.clear()
        
        # ãƒ˜ãƒƒãƒ€ãƒ¼ + ãƒ‡ãƒ¼ã‚¿ã‚’ä¸€æ‹¬æŠ•å…¥
        headers = combined_df.columns.tolist()
        all_data = [headers] + combined_df.values.tolist()
        
        worksheet.update('A1', all_data, value_input_option='USER_ENTERED')
        print(f"âœ… {sheet_type}æ›´æ–°å®Œäº†: ç·è¨ˆ{len(combined_df)}è¡Œï¼ˆæ–°è¦è¿½åŠ : {len(unique_new_df)}è¡Œï¼‰")
        
        return len(unique_new_df)
        
    except Exception as e:
        print(f"âŒ {sheet_type}è¿½è¨˜ã‚¨ãƒ©ãƒ¼: {e}")
        import traceback
        traceback.print_exc()
        return 0

def upload_to_google_sheets_func(fishing_csv_filename, comment_csv_filename):
    """Google SheetsæŠ•å…¥ï¼ˆè¿½è¨˜æ©Ÿèƒ½å¯¾å¿œç‰ˆï¼‰"""
    try:
        print(f"ğŸ“¤ Google SheetsæŠ•å…¥é–‹å§‹ï¼ˆè¿½è¨˜ç‰ˆï¼‰")
        print(f"  é‡£æœãƒ‡ãƒ¼ã‚¿: {fishing_csv_filename}")
        print(f"  ã‚³ãƒ¡ãƒ³ãƒˆ: {comment_csv_filename}")
        
        # Google Sheetsã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–
        client, error = setup_google_sheets_client()
        if error:
            print(f"âŒ Google Sheetsã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–ã‚¨ãƒ©ãƒ¼: {error}")
            return {'success': False, 'error': error}
        
        print(f"âœ… Google Sheetsã‚¯ãƒ©ã‚¤ã‚¢ãƒ³ãƒˆåˆæœŸåŒ–å®Œäº†")
        
        # ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆå–å¾—ãƒ»ä½œæˆ
        spreadsheet_name = "æœ¬ç‰§æµ·é‡£ã‚Šæ–½è¨­ãƒ‡ãƒ¼ã‚¿"
        try:
            spreadsheet = client.open(spreadsheet_name)
            print(f"âœ… æ—¢å­˜ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆä½¿ç”¨: {spreadsheet_name}")
            
            # æ—¢å­˜ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã«ã‚‚å…±æœ‰è¨­å®šã‚’é©ç”¨
            try:
                spreadsheet.share(None, perm_type='anyone', role='writer')
                print(f"âœ… æ—¢å­˜ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚’ã€Œãƒªãƒ³ã‚¯ã‚’çŸ¥ã£ã¦ã„ã‚‹å…¨å“¡ã€ã«å…±æœ‰ã—ã¾ã—ãŸ")
            except Exception as share_error:
                print(f"âš ï¸ å…±æœ‰è¨­å®šã‚¨ãƒ©ãƒ¼: {share_error}")
                
        except:
            spreadsheet = client.create(spreadsheet_name)
            print(f"âœ… æ–°è¦ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆä½œæˆ: {spreadsheet_name}")
            
            # ãƒªãƒ³ã‚¯ã‚’çŸ¥ã£ã¦ã„ã‚‹å…¨å“¡ãŒã‚¢ã‚¯ã‚»ã‚¹å¯èƒ½ã«è¨­å®š
            try:
                spreadsheet.share(None, perm_type='anyone', role='writer')
                print(f"âœ… ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆã‚’ã€Œãƒªãƒ³ã‚¯ã‚’çŸ¥ã£ã¦ã„ã‚‹å…¨å“¡ã€ã«å…±æœ‰ã—ã¾ã—ãŸ")
            except Exception as share_error:
                print(f"âš ï¸ å…±æœ‰è¨­å®šã‚¨ãƒ©ãƒ¼: {share_error}")
        
        results = {
            'success': True,
            'spreadsheet_url': f"https://docs.google.com/spreadsheets/d/{spreadsheet.id}",
            'message': f"Google SheetsæŠ•å…¥å®Œäº†",
            'fishing_count': 0,
            'comment_count': 0
        }
        
        # é‡£æœãƒ‡ãƒ¼ã‚¿å‡¦ç†ï¼ˆè¿½è¨˜ç‰ˆï¼‰
        if fishing_csv_filename and os.path.exists(fishing_csv_filename):
            print("\nğŸ£ é‡£æœãƒ‡ãƒ¼ã‚¿å‡¦ç†é–‹å§‹...")
            new_fishing_df = pd.read_csv(fishing_csv_filename, encoding='utf-8')
            
            try:
                fishing_worksheet = spreadsheet.worksheet("é‡£æœãƒ‡ãƒ¼ã‚¿")
                print("ğŸ“‹ æ—¢å­˜ã®é‡£æœãƒ‡ãƒ¼ã‚¿ã‚·ãƒ¼ãƒˆã‚’ä½¿ç”¨")
            except:
                fishing_worksheet = spreadsheet.add_worksheet(title="é‡£æœãƒ‡ãƒ¼ã‚¿", rows=1000, cols=20)
                print("âœ… æ–°è¦é‡£æœãƒ‡ãƒ¼ã‚¿ã‚·ãƒ¼ãƒˆä½œæˆ")
            
            # è¿½è¨˜å‡¦ç†
            added_fishing = append_data_to_worksheet(fishing_worksheet, new_fishing_df, "é‡£æœãƒ‡ãƒ¼ã‚¿")
            results['fishing_count'] = added_fishing
        
        # ã‚³ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿å‡¦ç†ï¼ˆè¿½è¨˜ç‰ˆï¼‰
        if comment_csv_filename and os.path.exists(comment_csv_filename):
            print("\nğŸ’¬ ã‚³ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿å‡¦ç†é–‹å§‹...")
            new_comment_df = pd.read_csv(comment_csv_filename, encoding='utf-8')
            
            try:
                comment_worksheet = spreadsheet.worksheet("ã‚³ãƒ¡ãƒ³ãƒˆ")
                print("ğŸ“‹ æ—¢å­˜ã®ã‚³ãƒ¡ãƒ³ãƒˆã‚·ãƒ¼ãƒˆã‚’ä½¿ç”¨")
            except:
                comment_worksheet = spreadsheet.add_worksheet(title="ã‚³ãƒ¡ãƒ³ãƒˆ", rows=1000, cols=20)
                print("âœ… æ–°è¦ã‚³ãƒ¡ãƒ³ãƒˆã‚·ãƒ¼ãƒˆä½œæˆ")
            
            # è¿½è¨˜å‡¦ç†
            added_comment = append_data_to_worksheet(comment_worksheet, new_comment_df, "ã‚³ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿")
            results['comment_count'] = added_comment
        
        # çµæœãƒ¡ãƒƒã‚»ãƒ¼ã‚¸æ›´æ–°
        total_added = results['fishing_count'] + results['comment_count']
        results['message'] = f"Google Sheetsè¿½è¨˜å®Œäº†: é‡£æœ{results['fishing_count']}è¡Œ, ã‚³ãƒ¡ãƒ³ãƒˆ{results['comment_count']}è¡Œ (æ–°è¦è¿½åŠ è¨ˆ{total_added}è¡Œ)"
        
        print(f"\nğŸ“Š è¿½è¨˜çµæœ:")
        print(f"   - é‡£æœãƒ‡ãƒ¼ã‚¿: {results['fishing_count']}è¡Œè¿½åŠ ")
        print(f"   - ã‚³ãƒ¡ãƒ³ãƒˆãƒ‡ãƒ¼ã‚¿: {results['comment_count']}è¡Œè¿½åŠ ")
        print(f"   - åˆè¨ˆ: {total_added}è¡Œè¿½åŠ ")
        print(f"ğŸ”— ã‚¹ãƒ—ãƒ¬ãƒƒãƒ‰ã‚·ãƒ¼ãƒˆURL: {results['spreadsheet_url']}")
        print(f"ğŸ“‹ ãƒ–ãƒ©ã‚¦ã‚¶ã§ç¢ºèªã—ã¦ãã ã•ã„")
        
        return results
        
    except Exception as e:
        error_msg = f"Google SheetsæŠ•å…¥ã‚¨ãƒ©ãƒ¼: {e}"
        print(f"âŒ {error_msg}")
        import traceback
        print("ğŸ“‹ ã‚¨ãƒ©ãƒ¼è©³ç´°:")
        traceback.print_exc()
        return {'success': False, 'error': error_msg}

def run_scraping(target_year_month, headless=True, upload_to_sheets=True):
    """ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œï¼ˆGoogle SheetsæŠ•å…¥ç‰ˆï¼‰"""
    scraper = ScrapingCore(headless=headless)
    
    try:
        # WebDriveråˆæœŸåŒ–
        if not scraper.setup_driver():
            return {'success': False, 'error': 'WebDriveråˆæœŸåŒ–å¤±æ•—'}
        
        # ã‚µã‚¤ãƒˆã‚¢ã‚¯ã‚»ã‚¹
        if not scraper.access_site():
            return {'success': False, 'error': 'ã‚µã‚¤ãƒˆã‚¢ã‚¯ã‚»ã‚¹å¤±æ•—'}
        
        # ã‚¹ã‚¯ãƒ¬ã‚¤ãƒ”ãƒ³ã‚°å®Ÿè¡Œ
        result = scraper.scrape_period(target_year_month)
        
        if not result['success']:
            return result
        
        # Google SheetsæŠ•å…¥å‡¦ç†
        if upload_to_sheets and result['total_records'] > 0:
            print("\n" + "="*60)
            print("ğŸ“¤ Google SheetsæŠ•å…¥å‡¦ç†é–‹å§‹")
            print("="*60)
            
            sheets_result = upload_to_google_sheets_func(
                result['fishing_csv'], 
                result['comment_csv']
            )
            
            result['sheets_result'] = sheets_result
        elif upload_to_sheets and result['total_records'] == 0:
            print("âš ï¸ ãƒ‡ãƒ¼ã‚¿ãŒ0ä»¶ã®ãŸã‚Google SheetsæŠ•å…¥ã‚’ã‚¹ã‚­ãƒƒãƒ—ã—ã¾ã—ãŸ")
            result['sheets_result'] = {'success': True, 'message': 'ãƒ‡ãƒ¼ã‚¿ãªã—ã®ãŸã‚ã‚¹ã‚­ãƒƒãƒ—'}
        else:
            print("âš ï¸ Google SheetsæŠ•å…¥ãŒã‚¹ã‚­ãƒƒãƒ—ã•ã‚Œã¾ã—ãŸï¼ˆ--no-sheetsã‚ªãƒ—ã‚·ãƒ§ãƒ³ï¼‰")
            result['sheets_result'] = {'success': True, 'message': 'ã‚¹ã‚­ãƒƒãƒ—'}
        
        return result
        
    except Exception as e:
        return {'success': False, 'error': str(e)}
    finally:
        scraper.cleanup()